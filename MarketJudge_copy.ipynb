{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Crimsonking96/Complete-Python-3-Bootcamp/blob/master/MarketJudge_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0486a979",
      "metadata": {
        "tags": [
          "Outline"
        ],
        "id": "0486a979"
      },
      "source": [
        "### The goal of this pipeline is to predict the next days movements (which sectors are hot and such)\n",
        "\n",
        "- This block (successor to statement) tells us how current markets are positioned\n",
        "- This information will then be fed into a system that will recognise positions of certain companys (their place in the market) and will convert the information (given my conditions) into a weight\n",
        "- That weight will then be used in the Neural network of a SEMI-SUPERVISED ALGORITHM (The Judge), along with weightings of current Economy (interest rates, inflation, etc) it will then follow a Markovian Decision process (potentially a MCMC)\n",
        "    - A dataset containg all the values of NYSE will be used to train the Judge\n",
        "    - We'll also have scrapers using real time News to be used to Predict tomorrows outcomes\n",
        "    - The Judge will be trained on a range of mid-cap-sized stocks from the afforementioned pool\n",
        "    - It can identify and sort the stocks with the most growth (before as well as) following the pandemic, and those that are deemed volatile (determined using either the black scholes equation, or the Bachelier equation)\n",
        "    - A Monte Carlo model will then be used to estimate a REASONABLE pot on Vulnerabilities (\"Not just how to grow, but when\"/ Should it Short, if so what ratio (warrant to stock)? Long call?)\n",
        "- The Judge will then purchase these stocks on a simulation (investopedia)\n",
        "    - Once Purchased the Machine will hold the stock until certain requirements have been broken, such as the stock dropping a significant percentage (1.33 points from what is deemed a peak). IT MUST SELL WHILE THE STOCKS ARE HOT!\n",
        "- Once the simulation has been proven, I'll need to devise a way for the Judge to inreact with my portfolio\n",
        "\n",
        "In RECAP, the Judge is a day trading model that will use my specific rules to buy/short and sell \"Hot Stocks\" and their respected Warrants (Mid Cap stocks with \"potential\" to grow or fall) based off of predictions of previouse market sector movements and current events. Although it uses data based off of the NYSE, it's ablility to identify which sectors will be profitable will allow it to identify Global stock prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e84a78c",
      "metadata": {
        "tags": [
          "Libraries",
          "Machine_learning"
        ],
        "id": "7e84a78c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import csv\n",
        "\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import time\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.axes as ax\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "\n",
        "from scipy import *\n",
        "\n",
        "import sklearn\n",
        "from sklearn import *\n",
        "from sklearn.preprocessing import StandardScaler as stscale\n",
        "from sklearn.preprocessing import normalize as normal\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "#Scikit\n",
        "from sklearn import *\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#TensorFlow\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model, load_model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.spatial.distance import cityblock, euclidean, cosine\n",
        "import nltk, re, random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from preprocessing import *\n",
        "\n",
        "#Pytorch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from torch import nn\n",
        "from skorch import NeuralNetClassifier\n",
        "\n",
        "import pytensor\n",
        "from pytensor import tensor as pt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b004d4e",
      "metadata": {
        "id": "6b004d4e"
      },
      "source": [
        "### Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c75f30c0",
      "metadata": {
        "tags": [
          "SnP_sectors",
          "Selenium/NLTK",
          "use_BS_instead"
        ],
        "id": "c75f30c0"
      },
      "outputs": [],
      "source": [
        "def extract_sector_data(url):\n",
        "    options = webdriver.EdgeOptions()\n",
        "    driver = webdriver.Edge(options=options)\n",
        "    driver.get(url)\n",
        "\n",
        "    # Wait for the table to be present and visible\n",
        "    wait = WebDriverWait(driver, 10)  # Adjust timeout as needed\n",
        "    table = wait.until(EC.visibility_of_element_located((By.TAG_NAME, \"table\")))\n",
        "\n",
        "    # Extract data from table rows\n",
        "    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
        "    data = []\n",
        "    for row in rows[1:]:  # Skip the header row\n",
        "        cells = row.find_elements(By.TAG_NAME, \"td\")\n",
        "        row_data = [cell.text for cell in cells]\n",
        "        data.append(row_data)\n",
        "\n",
        "    driver.quit()\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "url = \"https://www.sectorspdrs.com/allsectors\"\n",
        "sector_data = extract_sector_data(url)\n",
        "\n",
        "# Process and print the extracted data (example)\n",
        "for row in sector_data:\n",
        "#     print(word_tokenize(cell) for cell in row])\n",
        "#     for cell in row:\n",
        "#         word_tokenize(cell)\n",
        "#     print('\\n' f'{row[1]} ({row[0]}), {row[4]} volume:({row[5]})')\n",
        "    print(f'{row[0]}, {row[2]}, {row[3]} volume:({row[5]})')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3776269",
      "metadata": {
        "tags": [
          "SnP_sectors",
          "Selenium/NLTK",
          "use_BS_instead"
        ],
        "id": "d3776269"
      },
      "outputs": [],
      "source": [
        "#Make this autonomous ^\n",
        "\n",
        "\n",
        "sectors = pd.DataFrame([\n",
        "    ['Symbol', 'Volume', 'Price'],\n",
        "    ['XLC', '3.6M', 83.26],\n",
        "    ['XLY', '3.42M', 175.97],\n",
        "    ['XLP', '11.87M', 77.35],\n",
        "    ['XLE', '16.25M', 93.22],\n",
        "    ['XLF', '38.66M', 41.67],\n",
        "    ['XLV', '7.95M', 143.70],\n",
        "    ['XLI', '9.7M', 123.51],\n",
        "    ['XLB', '4.1M', 91.54],\n",
        "    ['XLRE', '7.95M',  38.04],\n",
        "    ['XLK', '6.04M', 210.16],\n",
        "    ['XLU', '12.39M', 72.73]\n",
        "])\n",
        "\n",
        "sectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a0cbc32",
      "metadata": {
        "scrolled": false,
        "tags": [
          "SnP_sectors",
          "Selenium/NLTK",
          "use_BS_instead"
        ],
        "id": "9a0cbc32"
      },
      "outputs": [],
      "source": [
        "equities = pd.read_csv('NYSE_Catalog_3.csv')\n",
        "\n",
        "# Convert to numeric, removing dollar signs and commas\n",
        "equities['mkt_cap'] = equities['mkt_cap'].astype(str).str.replace(r'[$,]', '', regex=True)\n",
        "equities['mkt_cap'] = pd.to_numeric(equities['mkt_cap'], errors='coerce')\n",
        "\n",
        "equities['p/e'] = equities['p/e'].astype(str).str.replace(r'[$,]', '', regex=True)\n",
        "equities['p/e'] = pd.to_numeric(equities['p/e'], errors='coerce')\n",
        "\n",
        "equities['avg_volume'] = equities['avg_volume'].astype(str).str.replace(r'[$,]', '', regex=True)\n",
        "equities['avg_volume'] = pd.to_numeric(equities['avg_volume'], errors='coerce')\n",
        "\n",
        "equities['previous_close'] = equities['previous_close'].astype(str).str.replace(r'[$,]', '', regex=True)\n",
        "equities['previous_close'] = pd.to_numeric(equities['previous_close'], errors='coerce')\n",
        "\n",
        "equities['open'] = equities['open'].astype(str).str.replace(r'[$,]', '', regex=True)\n",
        "equities['open'] = pd.to_numeric(equities['open'], errors='coerce')\n",
        "\n",
        "equities['high'] = equities['high'].astype(str).str.replace(r'[$,]', '', regex=True)\n",
        "equities['high'] = pd.to_numeric(equities['high'], errors='coerce')\n",
        "\n",
        "equities['low'] = equities['low'].astype(str).str.replace(r'[$,]', '', regex=True)\n",
        "equities['low'] = pd.to_numeric(equities['low'], errors='coerce')\n",
        "\n",
        "equities['52_wkH'] = equities['52_wkH'].astype(str).str.replace(r'[$,]', '', regex=True)\n",
        "equities['52_wkH'] = pd.to_numeric(equities['52_wkH'], errors='coerce')\n",
        "\n",
        "equities['52_wkL'] = equities['52_wkL'].astype(str).str.replace(r'[$,]', '', regex=True)\n",
        "equities['52_wkL'] = pd.to_numeric(equities['52_wkL'], errors='coerce')\n",
        "\n",
        "# Handling missing or invalid values (if necessary)\n",
        "equities.fillna(0, inplace=True)\n",
        "equities.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Filtering the DataFrame\n",
        "mid_cap_stocks = equities[(equities['mkt_cap'] >= 10**9) & (equities['mkt_cap'] <= (10**9)*30)]\n",
        "\n",
        "mid_cap_stocks.to_csv('mid_cap_stocks.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7501e077",
      "metadata": {
        "id": "7501e077"
      },
      "outputs": [],
      "source": [
        "mid_cap_stocks.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "157435bf",
      "metadata": {
        "scrolled": true,
        "id": "157435bf"
      },
      "outputs": [],
      "source": [
        "# 1. Style and Theme\n",
        "sns.set_theme(style=\"whitegrid\")  # Use a clean, whitegrid style\n",
        "plt.figure(figsize=(12, 8))        # Adjust figure size as needed\n",
        "\n",
        "# 2. Customized Color Palette\n",
        "num_industries = equities['industry'].nunique()\n",
        "colors = sns.color_palette(\"husl\", n_colors=num_industries)  # Use \"husl\" for vibrant, distinct colors\n",
        "\n",
        "# 3. Scatter Plot with Refined Aesthetics\n",
        "sns.scatterplot(\n",
        "    data=equities, x='mkt_cap', y='avg_volume', hue='industry',\n",
        "    size='mkt_cap', sizes=(50, 800), alpha=0.7,  # Increase sizes, add transparency for overlap\n",
        "    palette=colors, edgecolor='black', linewidth=0.5  # Add black edges to points\n",
        ")\n",
        "\n",
        "# 4. Enhanced Axis Labels and Title\n",
        "plt.xlabel('Market Capitalization (in Billions USD)', fontsize=12)\n",
        "plt.ylabel('Average Daily Volume (in Millions)', fontsize=12)\n",
        "plt.title('Market Capitalization vs. Average Volume by Industry (Mid-Cap Stocks)', fontsize=14)\n",
        "\n",
        "# 5. Logarithmic X-Axis for Better Visualization (if data is skewed)\n",
        "plt.xscale('log')  # Use logarithmic scale if market caps vary widely\n",
        "\n",
        "# 6. Refined Legend Placement\n",
        "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Industry', fontsize=10)\n",
        "for text in legend.get_texts():\n",
        "    text.set_fontsize(10)  # Reduce legend font size for better aesthetics\n",
        "\n",
        "# 7. Grid Adjustments\n",
        "sns.despine(left=True, bottom=True)  # Remove top and right spines for cleaner look\n",
        "plt.grid(axis='y', linestyle='--')    # Show horizontal gridlines only\n",
        "\n",
        "# 8. Show the Plot\n",
        "plt.tight_layout()  # Adjust layout for better spacing\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4d17c20",
      "metadata": {
        "id": "f4d17c20"
      },
      "source": [
        "### scrape newsites, none in particular, We'll need web crawlers seeking specified info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ba34d55",
      "metadata": {
        "tags": [
          "dev_scraper_goog_fi"
        ],
        "id": "6ba34d55"
      },
      "outputs": [],
      "source": [
        "def get_stock_price(ticker, exchange):\n",
        "    \"\"\"Retrieves the current stock price for the given ticker and exchange from Google Finance.\n",
        "\n",
        "    Also handles printing the results (success or error).\n",
        "    \"\"\"\n",
        "\n",
        "    url = f\"https://www.google.com/finance/quote/{ticker}:{exchange}\"\n",
        "    html = urlopen(url)\n",
        "    bs = BeautifulSoup(html.read(), 'html.parser')\n",
        "\n",
        "    # Target the specific element containing the price\n",
        "    price_div = bs.find('div', class_='YMlKec fxKbKc')\n",
        "    if price_div:  # Check if price element is found\n",
        "        price_text = price_div.get_text()\n",
        "\n",
        "        # Convert to float (or handle cases where price isn't available)\n",
        "        try:\n",
        "            price = float(price_text.replace('$', ''))\n",
        "            print(f\"The current price of {ticker} is ${price:.2f}\")\n",
        "        except ValueError:\n",
        "            print(f\"Price information for {ticker} is not available at this time.\")\n",
        "    else:\n",
        "        print(f\"Price information for {ticker} is not available at this time.\")  # Handle missing price element\n",
        "\n",
        "get_stock_price('GCEHF', 'OTCMKTS')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2da20f3a",
      "metadata": {
        "tags": [
          "Financial_News"
        ],
        "id": "2da20f3a"
      },
      "outputs": [],
      "source": [
        "# Take newsource, place into df\n",
        "np.ndarray([\n",
        "    Title,\n",
        "    Traffick_count, #how many people have clciked the article; rows should be sorted largest to smallest\n",
        "    Word_count,\n",
        "    Date,\n",
        "    Url\n",
        "    ])\n",
        "\n",
        "#we'de like to be able to determine whether the article has a positive connotation or a negative one\n",
        "# ex: if word positive\n",
        "\n",
        "    # positive/word_count\n",
        "\n",
        "# else: negative/word_count\n",
        "\n",
        "#allowing us to comprehend what impact the article will have on the reader as well as some consensus as to what action they may succeed\n",
        "\n",
        "money_news = [\n",
        "    {'google_finance': 'Google Finance', 'url': 'https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGx6TVdZU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl', 'category': 'finance'},\n",
        "    {'yahoo_finance': 'Yahoo Finance', 'url': 'https://finance.yahoo.com/topic/stock-market-news/', 'category': 'finance'},\n",
        "    {'msn_finance': 'MSN Money', 'url': 'https://www.msn.com/en-us/money?id=a6qja2', 'category': 'finance'},\n",
        "    {'investopedia': 'Investopedia', 'url': 'https://www.investopedia.com/news-4427706', 'category': 'finance'},\n",
        "    {'pew_research': 'Pew Research Center', 'url': 'https://www.pewresearch.org/topic/economy-work/', 'category': 'research'}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb97807b",
      "metadata": {
        "tags": [
          "V.1"
        ],
        "id": "bb97807b"
      },
      "outputs": [],
      "source": [
        "# Data storage (using pandas DataFrame for now)\n",
        "data = pd.DataFrame(columns=[\"source\", \"url\", \"title\", \"summary\", \"tickers\", \"sentiment\"])\n",
        "\n",
        "money_news = [\n",
        "    {'google_finance': 'Google Finance', 'url': 'https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGx6TVdZU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl', 'category': 'finance'},\n",
        "    {'yahoo_finance': 'Yahoo Finance', 'url': 'https://finance.yahoo.com/topic/stock-market-news/', 'category': 'finance'},\n",
        "    {'msn_finance': 'MSN Money', 'url': 'https://www.msn.com/en-us/money?id=a6qja2', 'category': 'finance'},\n",
        "    {'investopedia': 'Investopedia', 'url': 'https://www.investopedia.com/news-4427706', 'category': 'finance'},\n",
        "    {'pew_research': 'Pew Research Center', 'url': 'https://www.pewresearch.org/topic/economy-work/', 'category': 'research'}\n",
        "]\n",
        "\n",
        "def crawl_page(source, url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract headlines and summaries (adapt to each website's structure)\n",
        "    headlines = soup.find_all('a', class_='DY5T1d RZIKme')  # Google Finance example\n",
        "    summaries = soup.find_all('div', class_='SbNwzf')      # Google Finance example\n",
        "\n",
        "    for headline, summary in zip(headlines, summaries):\n",
        "        title = headline.text\n",
        "        summary_text = summary.text\n",
        "\n",
        "        # Extract stock tickers\n",
        "        tickers = re.findall(r'\\b[A-Z]{1,6}\\b', title + \" \" + summary_text)\n",
        "        # Basic ticker pattern (uppercase, 1-6 letters), refine as needed\n",
        "\n",
        "        # Sentiment analysis\n",
        "        sentiment = TextBlob(summary_text).sentiment.polarity\n",
        "        if sentiment > 0:\n",
        "            sentiment = \"positive\"\n",
        "        elif sentiment < 0:\n",
        "            sentiment = \"negative\"\n",
        "        else:\n",
        "            sentiment = \"neutral\"\n",
        "\n",
        "        data.loc[len(data)] = [source, url, title, summary_text, tickers, sentiment]\n",
        "\n",
        "\n",
        "# Crawl through each source\n",
        "for source_dict in money_news:\n",
        "    source_name = list(source_dict.keys())[1]\n",
        "    source_url = source_dict[source_name]\n",
        "    crawl_page(source_name, source_url)\n",
        "\n",
        "print(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c504315d",
      "metadata": {
        "scrolled": true,
        "tags": [
          "V.2"
        ],
        "id": "c504315d"
      },
      "outputs": [],
      "source": [
        "# Data storage\n",
        "data = pd.DataFrame(columns=[\"source\", \"url\", \"title\", \"traffic_count\", \"word_count\", \"date\", \"tickers\", \"sentiment\"])\n",
        "\n",
        "money_news = [\n",
        "    {'google_finance': 'Google Finance', 'url': 'https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGx6TVdZU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl', 'category': 'finance'},\n",
        "    {'yahoo_finance': 'Yahoo Finance', 'url': 'https://finance.yahoo.com/topic/stock-market-news/', 'category': 'finance'},\n",
        "    {'msn_finance': 'MSN Money', 'url': 'https://www.msn.com/en-us/money?id=a6qja2', 'category': 'finance'},\n",
        "    {'investopedia': 'Investopedia', 'url': 'https://www.investopedia.com/news-4427706', 'category': 'finance'},\n",
        "    {'pew_research': 'Pew Research Center', 'url': 'https://www.pewresearch.org/topic/economy-work/', 'category': 'research'}\n",
        "]\n",
        "\n",
        "def crawl_page(source, url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Example selectors for Google Finance (adapt as needed)\n",
        "    articles = soup.find_all('article')\n",
        "\n",
        "    for article in articles:\n",
        "        try:\n",
        "            title_elem = article.find('a', class_='DY5T1d RZIKme')\n",
        "            title = title_elem.text.strip()\n",
        "\n",
        "            # Traffic count (replace with actual logic if available)\n",
        "            traffic_count = 0  # Placeholder, needs implementation\n",
        "\n",
        "            url = title_elem['href']\n",
        "            # Word count\n",
        "            summary_elem = article.find('div', class_='SbNwzf')\n",
        "            summary_text = summary_elem.text.strip()\n",
        "            word_count = len(summary_text.split())\n",
        "\n",
        "            # Date (replace with actual logic if available)\n",
        "            date = dt.datetime.today()  # Placeholder, needs implementation\n",
        "\n",
        "            # Tickers\n",
        "            tickers = re.findall(r'\\b[A-Z]{1,6}\\b', title + \" \" + summary_text)\n",
        "\n",
        "            # Sentiment\n",
        "            sentiment = TextBlob(summary_text).sentiment.polarity\n",
        "            sentiment_label = np.select([sentiment > 0, sentiment < 0], [\"positive\", \"negative\"], \"neutral\")\n",
        "\n",
        "            data.loc[len(data)] = [source, url, title, traffic_count, word_count, date, tickers, sentiment_label]\n",
        "\n",
        "        except (AttributeError, TypeError):\n",
        "            pass  # Handle cases where elements are not found\n",
        "\n",
        "# Crawl and sort by traffic count\n",
        "for source_dict in money_news:\n",
        "    source_name = list(source_dict.keys())[1]\n",
        "    source_url = source_dict[source_name]\n",
        "    print(crawl_page(source_name, source_url))\n",
        "\n",
        "# Sort by traffic count (descending)\n",
        "data.sort_values(by=\"traffic_count\", ascending=False, inplace=True)\n",
        "\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08390d07",
      "metadata": {
        "tags": [
          "twitter_sentiment_crawler"
        ],
        "id": "08390d07"
      },
      "outputs": [],
      "source": [
        "# additionally crawlers for Twitter may allow us to confirm what action the pre-established population may take\n",
        "# either by searching for terms relevent to the news source, or spotting additional clues as to what market actions are hot\n",
        "np.ndarray([\n",
        "    Topic, #what data was identified by the crawler\n",
        "    Traffick, #of the posts scraped, what was the percent of specific topics present\n",
        "    Hot, #must have a percent of over %75 to be considered for the training set\n",
        "    Date, #must be today\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b5a4829",
      "metadata": {
        "tags": [
          "Economics",
          "inflation",
          "interest_rates",
          "Bank_Loans"
        ],
        "id": "5b5a4829"
      },
      "outputs": [],
      "source": [
        "cpi_discrete =  pd.read_csv(\"CPILFESL.csv\") #consumer discretionary\n",
        "stky_shft = pd.read_csv(\"CORESTICKM159SFRBATL.csv\") #Consumer discretionary sticky cpi\n",
        "mrtge_30y = pd.read_csv(\"MORTGAGE30US.csv\") #30yr mortgage rate\n",
        "prime_loan = pd.read_csv(\"MPRIME.csv\") #top 25 banks short term loans\n",
        "ffr = pd.read_csv(\"DFF.csv\") #Federal funds rate\n",
        "unemplyment = pd.read_csv(\"UNRATE.csv\") #rate of Unemployment\n",
        "mdn_cpi = pd.read_csv(\"MEDCPIM158SFRBCLE.csv\") #Median CPI\n",
        "cpi_cty_avg = pd.read_csv(\"CPIAUCSL.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "338d2b18",
      "metadata": {
        "id": "338d2b18"
      },
      "source": [
        "### take the previously established data and convert them to weights for our ANN"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}